{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_BenBaccar_Lauzeral.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUPc4UL8dvTn/yuSPon4uZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lbenbaccar/Deep-Reinforcement-Learning-with-Double-Q-Learning/blob/main/RL_BenBaccar_Lauzeral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lpEx47Fs4HF"
      },
      "source": [
        "# Deep Reinforcement Learning with Double Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idxt0YOBrzju"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym.envs.registration import register\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1oEMa48tB2U"
      },
      "source": [
        "## Q-learning\n",
        "### Frozen lake problem [click here](https://gym.openai.com/envs/FrozenLake-v0/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyMN9hsns0I6"
      },
      "source": [
        "We derive the environment using the Gym library. Instead of using the original FrozenLake environment, we make a new one with no slippery to make sure the movement in the environment is deterministic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVUpuT97sjH-"
      },
      "source": [
        "class Environment():\n",
        "    def __init__(self):\n",
        "        # construct the environment where agent can perceive and act.\n",
        "        pass\n",
        "\n",
        "    def FrozenLakeNoSlippery(self):\n",
        "        # construct frozen lake without slippery\n",
        "        register(\n",
        "                 id= 'FrozenLakeNoSlippery-v0',\n",
        "                 entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "                 kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "                 max_episode_steps=100,\n",
        "                 reward_threshold=0.82\n",
        "                 )\n",
        "        env = gym.make('FrozenLakeNoSlippery-v0')\n",
        "        return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSXndFJ3P4m"
      },
      "source": [
        "#### Hyper-parameters and Q-table initialization \n",
        "- The discount factor is used to measure the importance of future reward. Its value is 0~1. The more closer to 1, the more important the future reward is.\n",
        "- The parameters exploration_rate and exploration_decay is used for training state and affects which kind of action, random action or optimal action, would the Agent take.\n",
        "- We create the simplest form of Q(s, a) — a matrix with states as rows and actions as columns, called Q-table/Q-matrix. Since the Agent knows nothing at the beginning, all Q-values are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BmFWbgQ9orn"
      },
      "source": [
        "#### Policy that agent follows\n",
        "During the training, the agent can take a random or optimal action. The former is for Exploration in order to get better future reward and the later is for Exploitation and always choose the best known action in a given state.\n",
        "\n",
        "During the testing, the agent always takes the optimal action.\n",
        "*texte en italique*\n",
        "- Optimal Policy (Exploitation) — choose the most valuable action in any state.\n",
        "- Random Policy (Exploration) — take a random action in given state.\n",
        "- Control the Exploration Rate — Start with 100% exploration, move slowly towards roughly 0% since we want less and less exploration and focus on the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z00xX0BjslZd"
      },
      "source": [
        "class QAgent():\n",
        "    def __init__(self, env):\n",
        "        # setting hyper-parameters and initialize Q-table\n",
        "        self.max_episodes = 20000   # set max training episodes\n",
        "        self.max_actions = 99       # set max actions per episodes\n",
        "        self.learning_rate = 0.83   # for q-learning\n",
        "        self.discount = 0.93        # for q-learning\n",
        "        self.exploration_rate = 1.0 # for exploration\n",
        "        self.exploration_decay = 1.0/self.max_episodes  # for exploitation\n",
        "\n",
        "        # get environmnent\n",
        "        self.env = env\n",
        "\n",
        "        # initialize Q(s, a)\n",
        "        row = env.observation_space.n # Discrete(16)\n",
        "        col = env.action_space.n # Discrete(4)\n",
        "        self.Q = np.zeros((row, col))\n",
        "\n",
        "    def _policy(self, mode, state, e_rate):\n",
        "        # return the action the Agent taken in a given state\n",
        "        if mode=='train':\n",
        "            if random.random() > e_rate:\n",
        "                return np.argmax(self.Q[state,:]) # exploitation\n",
        "            else:\n",
        "                return self.env.action_space.sample() # exploration\n",
        "        elif mode=='test':\n",
        "            return np.argmax(self.Q[state,:]) # optimal policy\n",
        "        \n",
        "    def train(self):\n",
        "        # training the agent\n",
        "        # get hyper-parameters\n",
        "        max_episodes = self.max_episodes\n",
        "        max_actions = self.max_actions\n",
        "        learning_rate = self.learning_rate\n",
        "        discount = self.discount\n",
        "        exploration_rate = self.exploration_rate\n",
        "        exploration_decay = 1.0/self.max_episodes\n",
        "        \n",
        "        # start training\n",
        "        for i in range(max_episodes):\n",
        "            state = self.env.reset() # reset the environment per eisodes\n",
        "            for a in range(max_actions):\n",
        "                action = self._policy('train', state, exploration_rate)\n",
        "                new_state, reward, done, info = self.env.step(action)\n",
        "                # The formulation of updating Q(s, a)\n",
        "                self.Q[state, action] = self.Q [state, action] + learning_rate*(reward+discount*np.max(self.Q [new_state, :]) - self.Q [state, action])\n",
        "                state = new_state # update the current state\n",
        "                if done == True:  # if fall in the hole or arrive to the goal, then this episode is terminated.\n",
        "                    break\n",
        "            if exploration_rate>0.001:\n",
        "                exploration_rate -= exploration_decay\n",
        "\n",
        "    def test(self):\n",
        "        # testing the agent\n",
        "        # Setting hyper-parameters\n",
        "        max_actions = self.max_actions\n",
        "        state = self.env.reset() # reset the environment\n",
        "        for a in range(max_actions):\n",
        "            self.env.render() # show the environment states\n",
        "            action = np.argmax(self.Q[state,:]) # take action with the Optimal Policy\n",
        "            new_state, reward, done, info = self.env.step(action) # arrive to next_state after taking the action\n",
        "            state = new_state # update current state\n",
        "            if done:\n",
        "                print(\"======\")\n",
        "                self.env.render()\n",
        "                break\n",
        "            print(\"======\")\n",
        "        self.env.close()\n",
        "        \n",
        "    def displayQ():\n",
        "        # show information\n",
        "        print(\"Q\\n\", self.Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y3Niql3SZ4l",
        "outputId": "f87a591d-a243-459f-90c0-6eac7050ae80"
      },
      "source": [
        "env = Environment().FrozenLakeNoSlippery() # construct the environment\n",
        "agent = QAgent(env) # get agent\n",
        "agent.train()\n",
        "print(\"Testing Model\")\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Model\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "======\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "======\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "======\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "======\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "======\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "======\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcZzyJlwVWyk"
      },
      "source": [
        "## Deep Q-Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFINSu0IV7Nb",
        "outputId": "c6b238ec-fff4-479f-f94f-85ece34e9c58"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8xfygyvWGRN"
      },
      "source": [
        "class DeepQAgent():\n",
        "    def __init__(self, env):\n",
        "        # set hyperparameters\n",
        "        self.max_episodes = 20000\n",
        "        self.max_actions = 99\n",
        "        self.discount = 0.93\n",
        "        self.exploration_rate = 1.0\n",
        "        self.exploration_decay = 1.0/20000\n",
        "        # get envirionment\n",
        "        self.env = env\n",
        "    \n",
        "        # nn_model parameters\n",
        "        self.in_units = env.observation_space.n\n",
        "        self.out_units = env.action_space.n\n",
        "        self.hidden_units = 10\n",
        "        \n",
        "        # construct nn model\n",
        "        self._nn_model()\n",
        "    \n",
        "        # save nn model\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "    def _nn_model(self):\n",
        "        self.a0 = tf.placeholder(tf.float32, shape=[1, self.in_units]) # input layer\n",
        "        self.y = tf.placeholder(tf.float32, shape=[1, self.out_units]) # ouput layer\n",
        "        \n",
        "        # from input layer to hidden layer\n",
        "        self.w1 = tf.Variable(tf.zeros([self.in_units, self.hidden_units], dtype=tf.float32)) # weight\n",
        "        self.b1 = tf.Variable(tf.random_uniform([self.hidden_units], 0, 0.01, dtype=tf.float32)) # bias\n",
        "        self.a1 = tf.nn.relu(tf.matmul(self.a0, self.w1) + self.b1) # the ouput of hidden layer\n",
        "        \n",
        "        # from hidden layer to output layer\n",
        "        self.w2 = tf.Variable(tf.zeros([self.hidden_units, self.out_units], dtype=tf.float32)) # weight\n",
        "        self.b2 = tf.Variable(tf.random_uniform([self.out_units], 0, 0.01, dtype=tf.float32)) # bias\n",
        "        \n",
        "        # Q-value and Action\n",
        "        self.a2 = tf.matmul(self.a1, self.w2) + self.b2 # the predicted_y (Q-value) of four actions\n",
        "        self.action = tf.argmax(self.a2, 1) # the agent would take the action which has maximum Q-value\n",
        "\n",
        "        # loss function\n",
        "        self.loss = tf.reduce_sum(tf.square(self.a2-self.y))\n",
        "        \n",
        "        # upate model\n",
        "        self.update_model =  tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(self.loss)\n",
        "\n",
        "    def train(self):\n",
        "        # get hyper parameters\n",
        "        max_episodes = self.max_episodes\n",
        "        max_actions = self.max_actions\n",
        "        discount = self.discount\n",
        "        exploration_rate = self.exploration_rate\n",
        "        exploration_decay = self.exploration_decay\n",
        "        \n",
        "        # start training\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer()) # initialize tf variables\n",
        "            for i in range(max_episodes):\n",
        "                state = env.reset() # reset the environment per eisodes\n",
        "                for j in range(max_actions):\n",
        "                     # get action and Q-values of all actions\n",
        "                    action, pred_Q = sess.run([self.action, self.a2],feed_dict={self.a0:np.eye(16)[state:state+1]})\n",
        "                    \n",
        "                    # if explorating, then taking a random action instead\n",
        "                    if np.random.rand()<exploration_rate: \n",
        "                        action[0] = env.action_space.sample() \n",
        "\n",
        "                    # get nextQ in given next_state\n",
        "                    next_state, rewards, done, info = env.step(action[0])\n",
        "                    next_Q = sess.run(self.a2,feed_dict={self.a0:np.eye(16)[next_state:next_state+1]})\n",
        "\n",
        "                    # update\n",
        "                    update_Q = pred_Q\n",
        "                    update_Q [0,action[0]] = rewards + discount*np.max(next_Q)\n",
        "                    \n",
        "                    sess.run([self.update_model],\n",
        "                             feed_dict={self.a0:np.identity(16)[state:state+1],self.y:update_Q})\n",
        "                    state = next_state\n",
        "                    \n",
        "                     # if fall in the hole or arrive to the goal, then this episode is terminated.\n",
        "                    if done:\n",
        "                        if exploration_rate > 0.001:\n",
        "                            exploration_rate -= exploration_decay\n",
        "                        break\n",
        "            # save model\n",
        "            save_path = self.saver.save(sess, \"./nn_model.ckpt\")\n",
        "\n",
        "    def test(self):\n",
        "        # get hyper-parameters\n",
        "        max_actions = self.max_actions\n",
        "        # start testing\n",
        "        with tf.Session() as sess:\n",
        "            # restore the model\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver=tf.train.import_meta_graph(\"./nn_model.ckpt.meta\") # restore model\n",
        "            saver.restore(sess, tf.train.latest_checkpoint('./'))# restore variables\n",
        "            \n",
        "            # testing result\n",
        "            state = env.reset()\n",
        "            for j in range(max_actions):\n",
        "                env.render() # show the environments\n",
        "                # always take optimal action\n",
        "                action, pred_Q = sess.run([self.action, self.a2],feed_dict={self.a0:np.eye(16)[state:state+1]})\n",
        "                # update\n",
        "                next_state, rewards, done, info = env.step(action[0])\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    env.render()\n",
        "                    break\n",
        "    def displayQ():\n",
        "        # show information\n",
        "        print(\"Q\\n\", self.Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpS68aYmWtLG",
        "outputId": "36d66c94-cbb0-4aa8-8f9c-cd5d0c5680d4"
      },
      "source": [
        "agent = DeepQAgent(env) # get agent\n",
        "print(\"START TRAINING...\")\n",
        "agent.train()\n",
        "print(\"\\n\\nTEST\\n\\n\")\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "START TRAINING...\n",
            "\n",
            "\n",
            "TEST\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from ./nn_model.ckpt\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbeS5cUuZBXA"
      },
      "source": [
        "## Double Deep Q Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvC8LyMQdIqr"
      },
      "source": [
        "### Cart Pole problem : [click here](https://gym.openai.com/envs/CartPole-v1/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq7FSBfpZ8hN"
      },
      "source": [
        "class ExpReplay():\n",
        "    def __init__(self, e_max=15000, e_min=100):\n",
        "        self._max = e_max # maximum number of experiences\n",
        "        self._min = e_min # minimum number of experiences for training\n",
        "        self.exp = {'state':[], 'action':[], 'reward':[], 'next_state':[], 'done':[]} # total experiences the Agent stored\n",
        "        \n",
        "    def get_max(self):\n",
        "        \"\"\"return the maximum number of experiences\"\"\"\n",
        "        return self._max\n",
        "    \n",
        "    def get_min(self):\n",
        "        \"\"\"return the minimum number of experiences\"\"\"\n",
        "        return self._min\n",
        "    \n",
        "    def get_num(self):\n",
        "        \"\"\"return the curren number of experiences\"\"\"\n",
        "        return len(self.exp['state'])\n",
        "    \n",
        "    def get_batch(self, batch_size=64):\n",
        "        \"\"\"random choose a batch of experiences for training\"\"\"\n",
        "        idx = np.random.choice(self.get_num(), size=batch_size, replace=False)\n",
        "        state = np.array([self.exp['state'][i] for i in idx])\n",
        "        action = [self.exp['action'][i] for i in idx]\n",
        "        reward = [self.exp['reward'][i] for i in idx]\n",
        "        next_state = np.array([self.exp['next_state'][i] for i in idx])\n",
        "        done = [self.exp['done'][i] for i in idx]\n",
        "        return state, action, reward, next_state, done\n",
        "        \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"remove the oldest experience if the meomory is full\"\"\"\n",
        "        if self.get_num()>self.get_max():\n",
        "            del self.exp['state'][0]\n",
        "            del self.exp['action'][0]\n",
        "            del self.exp['reward'][0]\n",
        "            del self.exp['next_state'][0]\n",
        "            del self.exp['done'][0]\n",
        "        \"\"\"add single experience\"\"\"\n",
        "        self.exp['state'].append(state)\n",
        "        self.exp['action'].append(action)\n",
        "        self.exp['reward'].append(reward)\n",
        "        self.exp['next_state'].append(next_state)\n",
        "        self.exp['done'].append(done)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwzRK1jraCu1"
      },
      "source": [
        "class TNET():\n",
        "    \"\"\"\n",
        "    Target network is for calculating the maximum estimated Q-value in given action a.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_units, out_units, hidden_units=250):\n",
        "        self.in_units = in_units\n",
        "        self.out_units = out_units\n",
        "        self.hidden_units = hidden_units\n",
        "        self._model()\n",
        "        \n",
        "    def _model(self):\n",
        "        with tf.variable_scope('tnet'):\n",
        "            # input layer\n",
        "            self.x = tf.placeholder(tf.float32, shape=(None, self.in_units))\n",
        "            \n",
        "            # from input layer to hidden layer1\n",
        "            W1=tf.get_variable('W1', shape=(self.in_units, self.hidden_units), initializer=tf.random_normal_initializer())\n",
        "            # from hidden layer1 to hiiden layer2\n",
        "            W2=tf.get_variable('W2', shape=(self.hidden_units, self.hidden_units), initializer=tf.random_normal_initializer())\n",
        "            # from hidden layer2 to output layer\n",
        "            W3=tf.get_variable('W3', shape=(self.hidden_units, self.out_units), initializer=tf.random_normal_initializer())\n",
        "            \n",
        "            # the bias of hidden layer1\n",
        "            b1=tf.get_variable('b1', shape=(self.hidden_units), initializer=tf.zeros_initializer())\n",
        "            # the bias of hidden layer2\n",
        "            b2=tf.get_variable('b2', shape=(self.hidden_units), initializer=tf.zeros_initializer())\n",
        " \n",
        "            # the ouput of hidden layer1\n",
        "            h1=tf.nn.tanh(tf.matmul(self.x, W1)+b1)\n",
        "            # the output of hidden layer2\n",
        "            h2=tf.nn.tanh(tf.matmul(h1, W2)+b2)\n",
        "            \n",
        "            # the output of output layer, that is, Q-value\n",
        "            self.q=tf.matmul(h2, W3)\n",
        "        \n",
        "            self.params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPpffmVZaGnw"
      },
      "source": [
        "class QNET():\n",
        "    def __init__(self, in_units, out_units, exp, hidden_units=250):\n",
        "        # Target Network\n",
        "        self.tnet = TNET(in_units, out_units)\n",
        "        \n",
        "        # Q network architecture\n",
        "        self.in_units = in_units\n",
        "        self.out_units = out_units\n",
        "        self.hidden_units = hidden_units\n",
        "        self._model()\n",
        "        self._batch_learning_model()\n",
        "        self._tnet_update()\n",
        "        \n",
        "        # experience replay\n",
        "        self.exp = exp \n",
        "        \n",
        "    def _model(self):\n",
        "        \"\"\" Q-network architecture \"\"\"\n",
        "        with tf.variable_scope('qnet'):\n",
        "            self.x = tf.placeholder(tf.float32, shape=(None, self.in_units))\n",
        "            \n",
        "            W1 = tf.get_variable('W1', shape=(self.in_units, self.hidden_units), initializer=tf.random_normal_initializer())\n",
        "            W2 = tf.get_variable('W2', shape=(self.hidden_units, self.hidden_units), initializer=tf.random_normal_initializer())\n",
        "            W3 = tf.get_variable('W3', shape=(self.hidden_units, self.out_units), initializer=tf.random_normal_initializer())\n",
        "            \n",
        "            b1 = tf.get_variable('b1', shape=(self.hidden_units), initializer=tf.zeros_initializer())\n",
        "            b2 = tf.get_variable('b2', shape=(self.hidden_units), initializer=tf.zeros_initializer())\n",
        " \n",
        "            h1 = tf.nn.tanh(tf.matmul(self.x, W1)+b1)\n",
        "            h2 = tf.nn.tanh(tf.matmul(h1, W2)+b2)\n",
        "            self.q = tf.matmul(h2, W3)\n",
        "\n",
        "    def _batch_learning_model(self):\n",
        "        \"\"\"For batch learning\"\"\"\n",
        "        with tf.variable_scope('qnet'):\n",
        "            # TD-target\n",
        "            self.target = tf.placeholder(tf.float32, shape=(None, ))\n",
        "            # Action index\n",
        "            self.selected_idx = tf.placeholder(tf.int32, shape=(None, 2))\n",
        "            # Q-value\n",
        "            self.selected_q = tf.gather_nd(self.q, self.selected_idx)\n",
        "            \n",
        "            self.params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='qnet')\n",
        "            \n",
        "            # Q-network optimization alogrithms\n",
        "            loss = tf.losses.mean_squared_error(self.target, self.selected_q)\n",
        "            gradients = tf.gradients(loss, self.params)\n",
        "            self.train_opt = tf.train.AdamOptimizer(3e-4).apply_gradients(zip(gradients, self.params))\n",
        "\n",
        "    def _tnet_update(self):\n",
        "        \"\"\" Update Target network by using the parameters of Q-Network\"\"\"\n",
        "        with tf.variable_scope('qnet'):                        \n",
        "            self.update_opt = [t.assign(q) for t, q in zip(self.tnet.params, self.params)]\n",
        "    \n",
        "    def batch_train(self, batch_size=64):\n",
        "        \"\"\"Implement Double DQN Algorithm, batch training\"\"\"\n",
        "        if self.exp.get_num() < self.exp.get_min():\n",
        "            #The number of experiences is not enough for batch training\n",
        "            return\n",
        "\n",
        "        # get a batch of experiences\n",
        "        state, action, reward, next_state, done = self.exp.get_batch(batch_size)\n",
        "        state = state.reshape(batch_size, self.in_units)\n",
        "        next_state = next_state.reshape(batch_size, self.in_units)\n",
        "        \n",
        "        # get actions by Q-network\n",
        "        qnet_q_values = self.session.run(self.q, feed_dict={self.x:next_state})\n",
        "        qnet_actions = np.argmax(qnet_q_values, axis=1)\n",
        "        \n",
        "        # calculate estimated Q-values with qnet_actions by using Target-network\n",
        "        tnet_q_values = self.session.run(self.tnet.q, feed_dict={self.tnet.x:next_state})\n",
        "        tnet_q = [np.take(tnet_q_values[i], qnet_actions[i]) for i in range(batch_size)]\n",
        "        \n",
        "        # Update Q-values of Q-network\n",
        "        qnet_update_q = [r+0.95*q if not d else r for r, q, d in zip(reward, tnet_q, done)]\n",
        "        \n",
        "        # optimization\n",
        "        indices=[[i,action[i]] for i in range(batch_size)]\n",
        "        feed_dict={self.x:state, self.target:qnet_update_q, self.selected_idx:indices}\n",
        "        self.session.run(self.train_opt, feed_dict)\n",
        "    \n",
        "    def update(self):\n",
        "        \"\"\" for updatte target network\"\"\"\n",
        "        self.session.run(self.update_opt)\n",
        "        \n",
        "    def set_session(self, sess):\n",
        "        self.session = sess\n",
        "        \n",
        "    def get_action(self, state, e_rate):\n",
        "        \"\"\" for training stage of the Agent, exploitation or exploration\"\"\"\n",
        "        if np.random.random()<e_rate:\n",
        "            return np.random.choice(self.out_units)\n",
        "        else:\n",
        "            return np.argmax(self.session.run(self.q, feed_dict={self.x: state}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zIqO12Mc-oI"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, env):\n",
        "        # set hyper parameters\n",
        "        self.max_episodes = 1000\n",
        "        self.max_actions = 10000\n",
        "        self.exploration_rate = 1.0\n",
        "        self.exploration_decay = 0.0001  \n",
        "        \n",
        "        # set environment\n",
        "        self.env = env\n",
        "        self.states = env.observation_space.shape[0]\n",
        "        self.actions = env.action_space.n\n",
        "        \n",
        "        # Experience Replay for batch learning\n",
        "        self.exp = ExpReplay()\n",
        "        # the number of experience per batch for batch learning\n",
        "        self.batch_size = 64 \n",
        "        \n",
        "        # Deep Q Network\n",
        "        self.qnet = QNET(self.states, self.actions, self.exp)\n",
        "        # For execute Deep Q Network\n",
        "        session = tf.InteractiveSession()\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        self.qnet.set_session(session)\n",
        "        \n",
        "    def train(self):\n",
        "        # set hyper parameters\n",
        "        max_episodes = self.max_episodes\n",
        "        max_actions = self.max_actions\n",
        "        exploration_rate = self.exploration_rate\n",
        "        exploration_decay = self.exploration_decay\n",
        "        batch_size = self.batch_size\n",
        "        \n",
        "        # start training\n",
        "        record_rewards = []\n",
        "        for i in range(max_episodes):\n",
        "            total_rewards = 0\n",
        "            state = self.env.reset()\n",
        "            state = state.reshape(1, self.states)\n",
        "            for j in range(max_actions):\n",
        "                #self.env.render() # Uncomment this line to render the environment\n",
        "                action = self.qnet.get_action(state, exploration_rate)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = next_state.reshape(1, self.states)\n",
        "                total_rewards += reward\n",
        "                \n",
        "                if done:\n",
        "                    self.exp.add(state, action, (reward-100), next_state, done)\n",
        "                    self.qnet.batch_train(batch_size)\n",
        "                    break\n",
        "                    \n",
        "                self.exp.add(state, action, reward, next_state, done)\n",
        "                self.qnet.batch_train(batch_size)\n",
        "                \n",
        "                # update target network\n",
        "                if (j%25)== 0 and j>0:\n",
        "                    self.qnet.update()\n",
        "                # next episode\n",
        "                state = next_state\n",
        "                \n",
        "            record_rewards.append(total_rewards)\n",
        "            exploration_rate = 0.01 + (exploration_rate-0.01)*np.exp(-exploration_decay*(i+1))\n",
        "            if i%100==0 and i>0:\n",
        "                average_rewards = np.mean(np.array(record_rewards))\n",
        "                record_rewards = []\n",
        "                print(\"episodes: %i to %i, average_reward: %.3f, exploration: %.3f\" %(i-100, i, average_rewards, exploration_rate))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31HJ6MvYdCuT",
        "outputId": "58d70f07-8256-4138-88c4-c416a4f81a8b"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "agent = Agent(env)\n",
        "agent.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episodes: 0 to 100, average_reward: 31.782, exploration: 0.601\n",
            "episodes: 100 to 200, average_reward: 159.830, exploration: 0.140\n",
            "episodes: 200 to 300, average_reward: 263.630, exploration: 0.021\n",
            "episodes: 300 to 400, average_reward: 260.130, exploration: 0.010\n",
            "episodes: 400 to 500, average_reward: 273.200, exploration: 0.010\n",
            "episodes: 500 to 600, average_reward: 332.550, exploration: 0.010\n",
            "episodes: 600 to 700, average_reward: 318.390, exploration: 0.010\n",
            "episodes: 700 to 800, average_reward: 367.220, exploration: 0.010\n",
            "episodes: 800 to 900, average_reward: 310.750, exploration: 0.010\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}